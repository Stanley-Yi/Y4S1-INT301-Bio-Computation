# INT301 W11

## INTRODUCTION TO PRINCIPAL COMPONENT ANALYSIS (PCA)

### Eigenvalues and Eigenvectors

如果 v 是一个非零向量，λ 是一个数字，并且：
$$
Av = λv
$$
那么 v 是 A 的 eigenvector (特征向量)，λ 是 A 的 eigenvalue (特征值)。

<img src="D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 Eigenvalues and Eigenvectors.png" alt="W11 Eigenvalues and Eigenvectors" style="zoom: 50%;" />

最多可以有多少特征值：
$ \mathbf{S v}=\lambda \mathbf{v} \Longleftrightarrow(\mathbf{S}-\lambda \mathbf{I}) \mathbf{v}=0 $
仅在  $|\mathbf{S}-\lambda \mathbf{I}|=0$ 时具有一个非零解，并最多具有 $m$ 个不同的 $\lambda$ 值 ( $m$ 是 v 的维度)。

注：$I$ 表示单位矩阵，即在主对角线上元素均为 1，而其他元素都是 0 的方阵。

对于对称矩阵 (symmetric matrices，指以主对角线为对称轴，各元素对应相等的矩阵)，特征值不同的特征向量是正交的：
$$
S v_{\{1,2\}}=\lambda_{\{1,2\}} v_{\{1,2\}}, \text { and } \lambda_{1} \neq \lambda_{2} \Rightarrow v_{1} \bullet v_{2}=0
$$
实对称矩阵 (real symmetric matrix，矩阵的元素都为实数) 的所有特征值都是实数。

正半定矩阵 (positive semidefinite matrix，设 A 为实对称矩阵，若对于每个非零实向量 X，都有 $X^{T}AX ≥ 0$，则称 A 为半正定矩阵) 的所有特征值都是非负的：
$$
\forall w \in \mathfrak{R}^{n}, w^{T} S w \geq 0, \text { then if } \mathrm{Sv}=\lambda \mathrm{v} \Rightarrow \lambda \geq 0
$$

#### Example

求 $S=\left[\begin{array}{ll}2 & 1 \\ 1 & 2\end{array}\right]$ 的特征值和特征向量。

由 $ \mathbf{S v}=\lambda \mathbf{v} \Longleftrightarrow(\mathbf{S}-\lambda \mathbf{I}) \mathbf{v}=0 $ 可以得到：

$$
(S - \lambda \mathbf{I}) \mathbf{v} = 0 \\

(\left[\begin{array}{ll}2 & 1 \\ 1 & 2\end{array}\right] -\lambda \left[\begin{array}{ll}1 & 0 \\ 0 & 1\end{array}\right]) \mathbf{v} = 0 \\
(\left[\begin{array}{ll}2-\lambda & 1-0 \\ 1-0 & 2-\lambda\end{array}\right])\mathbf{v} = 0 \\
[(2-\lambda)^{2} - 1] \mathbf{v} = 0
$$

因为 $\mathbf{v}$ 是非零向量，所以 $(2-\lambda)^{2} - 1 = 0$，因此 $\lambda = 1 \ 或 \ 3$。

由于 $S$ 是对称矩阵，因此其中特征值不同的特征向量是正交的：
$$
\mathbf{v}=\left(\begin{array}{c}
1 \\
-1
\end{array}\right) \ 或 \ \left(\begin{array}{l}
1 \\
1
\end{array}\right)
$$

### Diagonal Decomposition

设平方矩阵 $\mathbf{S} \in \mathbb{R}^{m \times m}$ 具有 m 个线性独立特征向量 (square matrix)。

Theorem: Exists an eigen decomposition (对角线分解)
$$
\mathbf{S}=\mathbf{U} \mathbf{\Lambda} \mathbf{U}^{-1}
$$
U 的列是 S 的特征向量，$\mathbf{\Lambda}$ 的对角元素是 S 的特征值 $\boldsymbol{\Lambda}=\operatorname{diag}\left(\lambda_{1}, \ldots, \lambda_{m}\right), \quad \lambda_{i} \geq \lambda_{i+1}$

![W11 Diagonal Decomposition 1](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 Diagonal Decomposition 1.png)

#### Example

![W11 Diagonal Decomposition 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 Diagonal Decomposition 2.png)

![W11 Diagonal Decomposition 3](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 Diagonal Decomposition 3.png)

### Symmetric Diagonal Decomposition

![W11 Symmetric Diagonal Decomposition](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 Symmetric Diagonal Decomposition.png)

### Singular Value Decomposition (SVD)

对一个秩 (rank) 为 k 的 m $\times$ n 的矩阵 A，存在一个因数分解 (factorization，奇异值分解 = SVD)：

<img src="D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 Singular Value Decomposition.png" alt="W11 Singular Value Decomposition" style="zoom: 50%;" />

U 的列是 $AA^{T}$ 的正交特征向量 (orthogonal eigenvectors)；

V 的列是 $A^{T}A$ 的正交特征向量 (orthogonal eigenvectors)；

$AA^{T}$ 的特征值 λ1 … λr 也是 $A^{T}A$ 的特征值：
$$
\begin{gathered}
\sigma_{i}=\sqrt{\lambda_{i}} \\
\sum=\operatorname{diag}\left(\sigma_{1} \ldots \sigma_{r}\right)，注：这个是奇异值 \ (Singular \ values)
\end{gathered}
$$
<img src="D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 Singular Value Decomposition 2.png" alt="W11 Singular Value Decomposition 2" style="zoom: 67%;" />

#### Example

![W11 Singular Value Decomposition 3](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 Singular Value Decomposition 3.png)

### Dimensionality Reduction

处理高维数据的一种方法是降低其维数，使用线性或非线性变换将高维数据投影到低维子空间。

<img src="D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 Dimensionality Reduction 1.png" alt="W11 Dimensionality Reduction 1" style="zoom: 67%;" />

线性变换易于计算：

<img src="D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 Dimensionality Reduction 2.png" alt="W11 Dimensionality Reduction 2" style="zoom:50%;" />

#### Process

简单来说，就是丢失一些信息，从而把数据从高维转到低维。

![W11 Dimensionality Reduction 3](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 Dimensionality Reduction 3.png)

![W11 Diagonal Decomposition 4](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 Diagonal Decomposition 4.png)

### Principal Component Analysis (PCA)

每一种降维技术都通过满足一定的标准(例如，信息丢失、数据识别等)来找到适当的转换。

PCA 的目标是降低数据的维数，同时尽可能多地保留数据集中存在的变化。

#### Principal Component 

PCA 顾名思义，就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。

主成分分析要先找到主成分。我们先看看最简单的情况，也就是 n=2，n'=1，也就是将数据从二维降维到一维。数据如下图。我们希望找到某一个维度方向，它可以代表这两个维度的数据。图中列了两个向量方向，u1 和 u2，其中 u1 比 u2 好。

<img src="D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 PCA.png" alt="W11 PCA" style="zoom:50%;" />

为什么 u1 比 u2 好呢？可以有两种解释，第一种解释是样本点到这个直线的距离足够近，第二种解释是样本点在这个直线上的投影能尽可能的分开。

为了方便计算，我们选择有**最大方差 (maximum variance)**的方向，作为 (第一) 主成分 (在多维中，主成分可能有很多个)，而其余的成分 (maximum residual variance) 的方向垂直于主成分。

<img src="D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 PCA 2.png" alt="W11 PCA 2" style="zoom: 50%;" />

#### Method

要进行主成分分析，先要找到主成分。根据最大方差来寻找。

方差的公式如下 (E 是 X 的期望)：
$$
\sigma^{2} = E\left(X^{2}\right)-E^{2}(X)
$$
我们假设：
$$
E[\mathbf{x}]=\mathbf{0}, \quad a=\mathbf{x}^{T} \mathbf{q}=\mathbf{q}^{T} \mathbf{x}, \quad\|\mathbf{q}\|=\left(\mathbf{q}^{T} \mathbf{q}\right)^{1 / 2}=1
$$
其中，q 就是我们要找的主成分。

因此：
$$
\begin{aligned}
\sigma^{2} &=E\left[a^{2}\right]-E[a]^{2}=E\left[a^{2}\right] \ (因为 \ E[\mathbf{x}]=\mathbf{0})\\
&=E\left[\left(\mathbf{q}^{T} \mathbf{x}\right)\left(\mathbf{x}^{T} \mathbf{q}\right)\right]=\mathbf{q}^{T} E\left[\mathbf{x} \mathbf{x}^{T}\right] \mathbf{q}=\mathbf{q}^{T} \mathbf{R q}
\end{aligned}
$$
上面的 R 就是我们的原始数据。因此，当 q 是 R 的主分量时，方差最大化。现在我们要找到最大的 q。

主成分 q 可以通过特征向量分解 (eigenvector decomposition) 得到：
$$
\mathbf{R}=\mathbf{Q} \boldsymbol{\Lambda} \mathbf{Q}^{T}, \\\quad \mathbf{Q}=\left[\mathbf{q}_{1}, \mathbf{q}_{2}, \ldots, \mathbf{q}_{j}, \ldots, \mathbf{q}_{m}\right], \ \boldsymbol{\Lambda}=\operatorname{diag}\left[\lambda_{1}, \lambda_{2}, \ldots, \lambda_{j}, \ldots, \lambda_{m}\right]
$$
其中，Q 中的 q1 就是第一主成分，q2 就是第二主成分 ...
$$
\Leftrightarrow \mathbf{R} \mathbf{q}_{j}=\lambda_{j} \mathbf{q}_{j} \quad j=1,2, \ldots, m \quad \Longrightarrow \mathbf{R q}=\lambda \mathbf{q}
$$
最后可以通过上式获得 q。

#### Advantage and Limitation

##### Advantage

* 减少原始数据的维度：减少训练过程中的时间消耗，提高效率

* 丢弃原始数据的某些信息：如果此信息是噪声

##### Limitation

* 丢弃原始数据的一些信息：如果丢弃的信息很重要，则不宜应用 PCA
* 主成分的含义：可能有较差的可解释性
* PCA的线性模型：不适用于非线性问题
* 假设第一主成分具有更高的重要性

## UNSUPERVISED LEARNING: HEBBIAN LEARNING & AE

无监督学习通过局部操作的一般规则发现输入数据中的重要特征或模式。

无监督学习网络通常由前馈连接和元素组成，以促进 ‘local’ learning。

### Hebbian Learning

当一个神经元反复激发另一个神经元时，后一个神经元的阈值降低，或者**神经元之间的突触权重增加**，实际上增加了第二个神经元激发的可能性。

Hebbian learning rule：$\Delta w_{\mathrm{ji}}=\eta y_{\mathrm{j}} x_{\mathrm{i}}$，$\eta$ 是学习率。

Hebb 规则中不需要期望或目标信号，因此它是无监督学习。

#### Weight update

考虑单个权重 w 的更新（x 和 y 是突触前和突触后的活动，即输入和输出）：
$$
\mathbf{w}(n+1)=\mathbf{w}(n)+\eta x(n) y(n)
$$
对于线性激活函数，假如 $y = \mathbf{w}^{T}\mathbf{x} = \mathbf{x}^{T}\mathbf{w}$：
$$
\mathbf{w}(n+1)=\mathbf{w}(n)[1+\eta x(n) x^{T}(n)]
$$
权重无限制地增加。如果初始权重为负，则它将在负数中增加；如果为正数，则将在正范围内增加。

#### Similarity measure

对于 y (输出)，点积可以写为：
$$
y=|\mathbf{w}||\mathbf{x}| \cos (\alpha)
$$
α = 向量 x 和 w 之间的角度

* 如果 α 近似值 0（x 和 w 很接近），则 y 很大
* 如果 α 近似值 90（x 和 w 很远），则 y 是 0

使用 Hebbian learning 训练的网络根据权重中包含的信息在其输入空间中创建相似性度量（内积）。权重在训练期间捕获（或记忆）数据中的信息。

在操作期间，当权重固定时，较大的输出 y 表示当前输入与在训练期间创建权重的输入 x "相似"。

### Oja’s Rule

简单的 Hebbian rule 会导致权重无限制地增加（或减少）。

权重需要 normalized 为 1：
$$
w_{j i}(n+1)=\frac{w_{j i}(n)+\eta x_{i}(n) y_{j}(n)}{\sqrt{\sum_{i}\left[w_{j i}(n)+\eta x_{i}(n) y_{j}(n)\right]^{2}}}
$$
Oja 证明了，对于很小的 η&lt;&lt;1，上述归一化可以近似为：
$$
w_{j i}(n+1)=w_{j i}(n)+\eta y_{j}(n)\left[x_{i}(n)-y_{j}(n) w_{j i}(n)\right]
$$
这就是 Oja's rule，或 normalized Hebbian rule。它涉及一个"遗忘项 (forgetting term)"，防止权重无限制地增长。其中 $w_{j i}$ 是第一个主成分。

Oja's rule 渐近收敛，不像 Hebbian rule 是不稳定的。

Oja's rule 在输入空间中创建一个主成分，作为应用于单个神经元时的权重向量。Oja's rule 可以被扩展以提取多个主成分。

#### Deflation method

Oja's rule 创建了主成分后，我们可以使用 deflation method 来找到其他成分 (垂直于主成分)。

Deflation method 的原理是从输入中减去主成分。

采用 deflation method 计算其他特征向量：

* 假设已经获得第一个成分 (w~1~)，计算第一个特征向量在输入上的投影：
  $$
  y=w_{1}^{T} x
  $$
  注：w~1~ 是一个向量，而 x 是一个方向，这两个相乘，就是 w~1~ 在 x 方向上的投影。

* 将修改后的输入生成为:
  $$
  \hat{x}=x-w_{1} y=x-w_{1} w_{1}^{T} x
  $$
  注：这是 deflation method 的核心步骤，就是从输入中减去主成分。$w_{1} y$ 是主成分，而 $\hat{x}$ 是剩余的成分，即 residual  component。

* 对修改后的数据重复 Oja's rule。

### PCA in Neural Networks

Multi-layer networks with bottleneck layer：

<img src="D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 PCA in Neural Networks 1.png" alt="W11 PCA in Neural Networks 1" style="zoom: 67%;" />

这里的 bottleneck layer 就是主成分，一共有 m 个，因此 W~L~ 跨过第一个 m 个主特征向量的子空间。

Train using auto-associative output: e = x – y，e 就是 error。使用反向传播进行无监督学习。

这里的训练是使 input 和 output 一致，以发现表征输入模式的重要特征。如果学习到最后，发现输入和输出一致，说明网络找的主成分都很重要。这可以通过学习 identity mapping 来实现，通过瓶颈传递数据：自动编码器 (auto-encoders)。

![W11 PCA in Neural Networks 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 PCA in Neural Networks 2.png)

### Auto-encoders

<img src="D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 Auto-encoders 1.png" alt="W11 Auto-encoders 1" style="zoom: 50%;" />

Auto-encoders 是一种前馈神经网络，它学习预测输出中的输入本身 (在输出层重现输入)。
$$
y^{(i)} = x^{(i)}
$$
输入到隐藏的部分对应于 encoder，隐藏到输出的部分对应于 decoder。

自动编码器倾向于找到类似于 PCA 的数据描述；而瓶颈层中的少量神经元充当 information compressor (code)。

#### Deep Auto-encoder

就是把 encoder 和 decoder 扩展为多层。

![W11 DeepAuto-encoder](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 DeepAuto-encoder.png)

#### Denoising Auto-encoder

通过添加随机噪声，它可以迫使自动编码器学习更强大的功能。

<img src="D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 Denoising Auto-encoder.png" alt="W11 Denoising Auto-encoder" style="zoom:80%;" />

注：$\mathbf{x}_i$ 到 $\tilde{\mathbf{x}}_{i}$ 添加了噪声，$\tilde{\mathbf{x}}_{i}$ 到 $\mathbf{h}_i$ 进行了降维，最后 $\hat{\mathbf{x}}_{i}$ 是输出。$\left\|\mathbf{x}_{i}-\hat{\mathbf{x}}_{i}\right\|_{2}^{2}$ 是 error。

The loss function：

![W11 Denoising Auto-encoder 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W11 Denoising Auto-encoder 2.png)

#### Auto-encoders Network

网络尝试在输出中重现输入，在隐藏层中诱导 short encoding。

此编码在较小的维度空间中保留有关输入的最大信息量，以便可以重建输入。

自动编码器网络可用于降维、压缩等。

