# Multilayer Perceptrons (MLP)

多层感知器（MLP） 是几个感知器的分层结构，它克服了单层网络的缺点。

MLP 神经网络能够学习非线性函数映射。

* 学习各种非线性决策表面（nonlinear decision surfaces）

Nonlinear functions can be represented by MLPs with
units that use nonlinear activation functions. （多层级 联系的 linear unit 仍然只生成线性映射）

## Differentiable Activation Functions

![W5 Differentiable Activation function](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W5 Differentiable Activation function.png)

![W5 Differentiable Activation 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W5 Differentiable Activation 2.png)

## Multilayer Network Structure

![W5 Multilayer Network Structure](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W5 Multilayer Network Structure.png)

![W5 Multilayer Network Structure 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W5 Multilayer Network Structure 2.png)

input signals（输入信号），最初是 input example，在逐层向前通过神经网络传播，这就是为什么它们经常被称为 **feedforward multilayer network**。

MLP 能力的属性：

* learning arbitrary functions
* learning continuous functions
* learning Boolean functions

## Backpropagation Learning Algorithm

MLP became applicable on practical tasks after the
discovery of a supervised training algorithm, the **error backpropagation learning algorithm**.

The error backpropagation algorithm includes two passes through the network:

* **forward pass**, and
* **backward pass**



![W5 Backpropagation Training](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W5 Backpropagation Training.png)

注：$$o_{i}$$ 代表第 i 个 input unit 的值（即输入）。

$$w_{ij}$$​ 代表从 input unit i 到 hidden unit j 的权重，$$w_{jk}$$​ 代表从 hidden unit j 到 output unit k 的权重。

![W5 Backpropagation Training 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W5 Backpropagation Training 2.png)

注：$$y_{k}$$ 指 output unit k 的 label。

![W5 Backpropagation Training 3](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W5 Backpropagation Training 3.png)



## On-line (incremental) learning

根据 example 进行修正 (revision) 叫 on-line leaning。

![W5 On-line training](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W5 On-line training.png)

注：这个和 Incremental Gradient Descent 类似，都是用单个 example 来更新权重。

## Derivation of Backpropagation Algorithm
![W5 Derivation of Backpropagation](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W5 Derivation of Backpropagation.png)

注：$$E_{e}$$ 是模型的 loss 函数。

![W5 Derivation of Backpropagation 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W5 Derivation of Backpropagation 2.png)

注：这里 s 和 w 不用角标，因为可以对 $$w_{jk}$$ 和 $$w_{ij}$$​​ 两个权重求梯度。

对 $$w_{jk}$$​​ ：$$\frac{\partial E_{e}}{\partial w_{jk}} = \frac{\partial E_{e}}{\partial o_{k}} \cdot \frac{\partial o_{k}}{\partial s_{k}} \cdot \frac{\partial s_{k}}{\partial w_{jk}}$$​​。

对 $$w_{ij}$$ ：$$\frac{\partial E_{e}}{\partial w_{ij}} = \frac{\partial E_{e}}{\partial o_{k}} \cdot \frac{\partial o_{k}}{\partial s_{k}} \cdot \frac{\partial s_{k}}{\partial o_{j}} \cdot \frac{\partial o_{j}}{\partial s_{j}} \cdot \frac{\partial s_{j}}{\partial w_{ij}}$$。

![W5 Derivation of Backpropagation 3](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W5 Derivation of Backpropagation 3.png)

![W5 Derivation of Backpropagation 4](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W5 Derivation of Backpropagation 4.png)

![W5 Derivation of Backpropagation 5](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W5 Derivation of Backpropagation 5.png)

注：上面这段是对 $$w_{jk}$$ 的。

![W5 Derivation of Backpropagation 6](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W5 Derivation of Backpropagation 6.png)

![W5 Derivation of Backpropagation 7](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W5 Derivation of Backpropagation 7.png)

![W5 Derivation of Backpropagation 8](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W5 Derivation of Backpropagation 8.png)



## Batch Backpropagation Algorithms

在每个 epoch 后计算 loss，这种叫做 batch learning。

![W5 Batch version of the BP](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W5 Batch version of the BP.png)

