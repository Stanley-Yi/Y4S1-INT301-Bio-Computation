# INT301 W2 Notes

## Data

A set of data records (also called examples, instances or cases) described by:

* kattributes: A1, A2, … Ak，如下图中：age，Has_Job，Own_House，Credit_Rating；
* a class: Each example is labelled with a pre-defined class，如下图 Class 中：Yes，和 No。

![W2 data](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W2 data.png)



## Fundamental assumption of learning

假设：training examples 的分布与 test examples 的分布相同（包括以后的 unseen examples）

对上面假设的强烈违反显然会导致分类准确性差。为了在 test data 上实现良好的准确性，training examples 必须充分代表 test data。



## Perceptron

感知器（perceptron）是神经网络，并可以使用错误校正规则（error-correcting rule）以"经验"改变。

注：上面这个意思就是，在网络出现错误的时候（比如分类错误），网络的权重就会改变（就是根据 loss 更新权重）。

最简单的感知器结构包括两层理想化的"神经元 (neurons)"，我们称之为 'units' of the network。这两层是：one layer of input units, and one layer of output units。

![W2 units of network](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W2 units of network.png)

**processing elements** of the perceptron are the **abstract neurons**.



The total input to the output unit $$j$$ is:
$$
S_{j}=\sum_{i=0}^{n} w_{j i} a_{i}
$$
$a_{i}:$​ input value from the i-th input unit 

$w_{j i}$​ : the weight of connection between $\mathrm{i}$​-th input and $\mathrm{j}$​​-th output units

注意：上式中，$$i$$ 是从 0 开始的。这是由于 input 层有一个 special bias input $$a_{0}$$。$$a_{0}$$ 的输入恒为 1，但是它的 $$w_{j0}$$ 会和其他权重一样变化。这个 special bias input $$a_{0}$$​ 相当于 b，后面会介绍。



和 MP neuron 类似，output unit $$j$$ 的输出 $$X_{j}$$ 也有一个阈值。

![W2 阈值](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W2 阈值.png)

最后，output 层的所有输出 $$X = \{X_{0}, X_{1}, ..., X_{n}\}$$​​​ 叫做 **output vector** of the network。



## Perceptron Training

Every processing element computes an output according its state and threshold:
$$
\begin{aligned}
&S_{j}=\sum_{i=0}^{n} w_{j i} a_{i} \\
&X_{j}=f\left(S_{j}\right)=\left\{\begin{array}{l}
1, S_{j} \geq \theta_{j} \\
0, S_{j}<\theta_{j}
\end{array}\right. \\
&e_{j}=\left(t_{j}-X_{j}\right)
\end{aligned}
$$
注：$$e_{j}$$ 是 error，即 target output (期望) 和 instant output (预测) 的差异。

### Perceptron Updating of the weights

在计算了每一个 output units 的 error 后：
$$
e_{j}=\left(t_{j}-X_{j}\right)
$$
现在需要对 weights 进行更新：

![W2 update weight](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W2 update weight.png)

这也被称为 **delta rule**：

![W2 delta rule](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W2 delta rule.png)



举个例子：

![W2 example](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W2 example.png)

![W2 example 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W2 example 2.png)

设 learning rate C = 0.25，banana 的 label = 1。

![W2 example 3](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W2 example 3.png)



The algorithm **converges (收敛)** to the correct classification, if：

* the training data is **linearly separable**;
* and the learning rate is sufficiently small,usually
  set below 1, which determines the **amount of correction made in a single iteration (单次迭代中的矫正次数)**.

对于任何 linearly separable 的数据集，learning rule 保证在有限数量的步骤中找到解决方案。



## Network Performance for Perceptron

![W2 Network Performance for Perceptron](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W2 Network Performance for Perceptron.png)



由于 target output 的值 $$t_{jp}$$，和 $$n_{p}，n_{o}$$ 的数量是常数，因此 RMS error 是一个只关于 $$X_{jp}$$ 的函数。

而 instant output $$X_{jp}$$ 是关于 input 值 $$a_{ip}$$ 和权重 $$W_{ji}$$ 的函数，$$a_{ip}$$ 也是常数，因此：

![W2 RMS](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W2 RMS.png)

RMS error 也是一个只关于连接权重 $$W_{ji}$$ 的函数。

注：$$a_{ip}$$ 指第 i 个输入的第 p 个 pattern。



网络的最佳性能与 RMS error 的最小值正相关，我们调整连接的权重以达到最小化 RMS error。



![W2 RMS on the Training or Testing Data](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W2 RMS on the Training or Testing Data.png)



## Perceptron As a Classifier

![W2 Perceptron as a classifier](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W2 Perceptron as a classifier.png)

注：$$w_{0}$$ 和之前的 $$a_{0}$$ 一样，就是 b。

![W2 Perceptron as a classifier 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W2 Perceptron as a classifier 2.png)



### Neural Network as Classifier

对于二分类，可以把网络的 output 当作一个 discriminant function y(x, w)（判断函数），当 x 在 class 1，y(x, w)  = 1；当 x 在 class 2，y(x, w)  = -1。

对于 m 个 classes，一个分类器（classifier）会把 feature space 分成 m 个 decision regions（决策区域）：

* 分割 class 的 line or curve 叫 decision boundary
* In more than 2 dimensions, this is a hyperplane



**The equation of the hyperplane is $$w \cdot x^{T} = 0$$.**



### Example of Perceptron Decision Boundary

Decision surface is the surface at which the output of the unit is precisely **equal to the threshold**, i.e. $$\sum w_{i} x_{i} = \theta$$​.

![W2 Example of Perceptron Decision Boundary 1](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W2 Example of Perceptron Decision Boundary 1.png)

![W2 Example of Perceptron Decision Boundary 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W2 Example of Perceptron Decision Boundary 2.png)

![W2 Example of Perceptron Decision Boundary 3](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W2 Example of Perceptron Decision Boundary 3.png)



## Linear Separability Problem

如果两个类的 patterns 可以通过以线性方程表示的 decision boundary 来划分:
$$
b + \sum^{n}_{i = 1} x_{i} w_{i} = 0
$$
那么，可以说它们是 linearly separable 的（反之，则是 linearly inseparable），感知器可以正确分类任何 patterns。

NOTE：如果没有 bias（即 b），hyperplane 会被迫和 origin（原点）相交。（这里的 b 和函数 y = ax + b 里的一样，就是让 hyperplane 可以移动）



### Examples of linearly inseparable classes

![W2 Examples of linearly inseparable classes](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W2 Examples of linearly inseparable classes.png)

![W2 Examples of linearly inseparable classes 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W2 Examples of linearly inseparable classes 2.png)

