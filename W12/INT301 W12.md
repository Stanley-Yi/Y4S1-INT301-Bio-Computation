# INT301 W12

## UNSUPERVISED LEARNING: COMPETITIVE LEARNING

### Unsupervised Competitive Learning

在 Hebbian networks 中，所有神经元可以同时"激发"。

而竞争性学习 (competitive learning) 意味着每个时间步长中，每组只有一个神经元被激发。这意味着输出单元相互竞争。

These are **winner-takes-all (WTA)** units.

竞争对神经网络很重要：

* 在生物神经系统中观察到神经元之间的竞争
* 竞争对于解决许多问题很重要

为了更好的理解，for example：

* 将一个输入模式进行分类，一共 m 类
  * 理想的情况：只有一个节点输出 1，其他所有节点都是 0
  * 不过，通常会有多个节点具有非零输出
  * 这时候可以引入竞争机制。如果这些类节点相互竞争，也许最终只有一个会赢，而所有其他节点都会输（winner-takes-all）。这样获胜者所代表的类就是最终的结果。

#### The architecture of competitive learning mechanism

<img src="D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W12 Unsupervised Competitive Learning.png" alt="W12 Unsupervised Competitive Learning" style="zoom:67%;" />

竞争学习机制的架构如上图：

* unit（激活的或非激活的）在图中表示为点
  * 激活的 unit 由黑点表示，非激活的由白点表示
* 在 layer 中的 unit 可以：
  * 接收来自上一次中所有单元的输入，将输出传递给下一层中的所有单元
* 层之间的连接是刺激性的 (excitatory)
* 层内的连接是抑制性的 (inhibitory):
  * 每层由一组相互抑制的 unit 的聚类组成
  * 集群中的 unit 相互抑制，使得每个聚类只有一个 unit 处于激活状态

### Winner-takes-all (WTA)

实现 WTA 的最简单方法：有一个外部的中央仲裁员（一个程序）通过比较竞争对手的当前输出来决定获胜者（任意地打破平局）。

这在生物学上是不合理的（在生物神经系统中不存在这样的外部仲裁员）。

### Simple Competitive Learning (SCL)

SCL 的过程如下：

* 初始化 k 个 prototype vectors (这里和 K-means 类似，K-means 是随机选 k 个中心，这里的 prototype vectors 也可以看作中心)
* 提供一个 example，即拿一个数据出来
* 把 example 和每一个 prototype vectors 的距离进行对比，离 example 最近的 prototype vectors 叫做 winner (这里和 k-means 又不一样，k-means 是把所有的数据都对比，而这里只对比一个)
* 将 winner 向 example 移动，移动多少距离可以自定义

直观清晰、合理的程序：

* 将原型放置在数据密度高的区域
* 识别最相关的 feature 组合

#### Winner:

Winner = 输出节点的传入权重，与输入向量的欧氏距离最短
$$
h_{j}=\sum_{i} w_{j i} x_{i} \\
w_{j^{*}} \cdot x \geq w_{j} \cdot x \quad \forall x
$$
注：第一个公式中，$h_{j}$ 是第 j 个输出节点的值，其中 w 是 prototype vectors，x 是数据，$w_{j i} x_{i}$ 用来衡量数据和 prototype vectors 的距离 (注意，这里是 inner product，即当两个向量的角度接近 0，那么 x 和 w 很接近，则结果很大；如果角度接近 90，那么 x 和 w 很远，则结果是 0)。

第二个公式中，这个公式是确定 winner 的。其中 $w_{j^{*}}$ 是 winner，$w_{j}$ 代指剩余的 unit。这个公式的意思就是，传入权重与输入向量的欧氏距离最短的那个 unit，就是 winner。

#### Update rule for all neurons

$$
\begin{aligned}
&\Delta w_{j^{*} i}=\eta y_{j}\left(x_{i}-w_{j^{*} i}\right) \\
&\left\{\begin{array}{l}
y_{j^{*}}=1 \\
y_{j}=0  \ \text { if } \ j \neq j^{*}
\end{array}\right.
\end{aligned}
$$

仅修改 winner 节点的传入权重。

除了上面这种，还可能有其他的 update rule：

<img src="D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W12 SCL.png" alt="W12 SCL" style="zoom: 50%;" />

#### SCL Example

![W12 SCL Example 1](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W12 SCL Example 1.png)

注：这里以 input vector 1 为例，先计算出 $\Delta w_{j^{*} i}$。由于 $\left\{\begin{array}{l}
y_{j^{*}}=1 \\
y_{j}=0  \ \text { if } \ j \neq j^{*}
\end{array}\right.$，因此我们只考虑当前的 winner。因此 $\Delta w_{j^{*} i} = 0.5 \times 1 \times (x_{i}-w_{j^{*} i})$，最后的结果是：[-0.07  0.125  -0.145]，然后用 winner 加上 $\Delta w_{j^{*} i}$，即得到结果。

![W12 SCL Example 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W12 SCL Example 2.png)

![W12 SCL Example 3](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W12 SCL Example 3.png)

以上方法，形成的聚类对初始权重向量高度敏感 (不同的初始 prototype vectors 会得到不同的聚类)。

#### Enforcing fairer competition

output unit 的权重矢量的初始位置可能位于模式很少的区域。

有些 unit 可能永远不会或很少成为 winner，因此权重矢量可能不会更新，从而阻止它找到更丰富的模式空间部分。***DEAD UNIT***

更有效率，以确保更公平的竞争，每个 unit 都有平等的机会代表训练数据的某些部分。

### Leaky learning

这个是对上面 SCL 的修改：同时更新 winner 和 losing (其余) unit 的权重，但学习率不同：
$$
w(t+1)=w(t)+\left\{\begin{array}{l}
\eta_{w}(x-w(t)) \\
\eta_{L}(x-w(t))
\end{array}\right. \\
where \ \eta_{w} \gg \eta_{L}
$$
它具有将 losing unit 缓慢地向更密集区域模式空间移动的效果。

### Maxnet

#### Lateral inhibition

执行 Winner Take All (WTA) 竞争的特定竞争网络是 Maxnet。

每个节点的输出通过抑制性连接（负权重）馈送到其他节点 (同一个 layer 之间)。

<img src="D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W12 Maxnet 1.png" alt="W12 Maxnet 1" style="zoom:67%;" />

#### Maxnet

竞争对手之间的横向抑制 (lateral inhibition)，下面这些都是同一层内的 unit。

![W12 Maxnet 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W12 Maxnet 2.png)

经过上面的步骤，最终只有一个 node 可以输出 1，其他的都是 0。

Notes：

* 竞争：进行迭代，直到网络稳定（最多一个具有正激活的节点）$0<\varepsilon<1 / \boldsymbol{m}$, where $m$ is the number of competitors
* $\varepsilon$ 太小：收敛时间太长
* $\varepsilon$ 太大：可能会抑制整个网络 (没有 winner)

### Mexican Hat Network

在所有竞争节点中，只有一个会赢，所有其他节点都会输。

* 在输出层的所有神经元中，激活水平最高的神经元成为赢家。该神经元是唯一产生输出信号的神经元。所有其他神经元的活动在竞争中被抑制。

我们主要处理单 Winner WTA，但多个 winners WTA 是可能的。

* 侧向连接产生刺激或抑制作用，这取决于与 winner 神经元的距离。

* 这是通过使用 Mexican Hat function 来实现的，该函数描述了输出层中神经元之间的突触权重。

#### Architecture

对于一个给定的 node (近交远攻)：

* close neighbors: cooperative (合作，相互刺激，w > 0)
* distant neighbors: competitive (竞争，相互抑制，w < 0)
* too far away neighbors: irrelevant (w = 0)

![W12 Mexican Hat Network 1](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W12 Mexican Hat Network 1.png)

需要一个距离（neighborhood）的定义：

* 一维：按索引排序（1，2, ... n）
* 二维：lattice (格子，可以当作距离)

均衡 (Equilibrium)：

* negative input = positive input for all nodes (刺激和抑制的数量相等)
* winner has the highest activation
* its cooperative neighbors all have positive activations
* its competitive neighbors all have negative (or zero) activations

![W12 Mexican Hat Network 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W12 Mexican Hat Network 2.png)

### Vector Quantization

Idea：

* 使用竞争学习算法将给定的输入向量集分为 M 类
* 仅通过其所属的类来表示任何向量

竞争学习的重要应用（特别是在数据压缩 (data compression) 中）：

* 将整个模式空间划分为多个单独的子空间
* M 个 units 的集合表示 prototype vectors 的集合 - ***CODEBOOK***
* 新模式 x 基于其与 prototype vectors 的接近程度，使用欧式距离分配给聚类

A codebook：

* a set of centroids/codewords/codevector：
  $$
  \{ m_{1}, \ m_{2}, \ ... \ m_{k} \}
  $$
  注：上面的元素都是每个类的 prototype vector

A quantization function (量化函数):
$$
q\left(x_{i}\right)=m_{k}
$$
根据上面的 quantization function，我们可以对 new pattern x 进行分类。方程 q 会找到 x 的 nearest-neighbor。

K-means 可用于构建 codebook。

![W12 VQ](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W12 VQ.png)

