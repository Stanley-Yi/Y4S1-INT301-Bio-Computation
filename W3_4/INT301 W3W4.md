# Gradient Descent Rule

![W34 gradient descent rule](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W34 gradient descent rule.png)

注：$$o_{e} = w_{i} x_{ie}$$

![W34 gradient descent rule 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W34 gradient descent rule 2.png)

![W34 gradient descent rule 3](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W34 gradient descent rule 3.png)

## Gradient Descent Learning Algorithm

![W34 Gradient Descent Learning Algorithm](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W34 Gradient Descent Learning Algorithm.png)

注意：这里 $$\Delta w$$ 需要积累，即要把所有 training examples 的都考虑进去。这是 gradient descent 的特点，而之后的 increasement gradient descent 就不用。

Example:

![W34 example](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W34 example.png)

注：$$x_{0}$$ 恒为 1

![W34 example 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W34 example 2.png)

注意，上面对 $$\Delta w$$ 进行了计算（蓝字），先把所有的 $$\Delta w$$ 都考虑到，最后再根据最终的 $$\Delta w$$ 进行权重更新。

## Incremental gradient descent

梯度下降规则在实践中面临两个困难：

* 它收敛非常缓慢
* 如果在 error surface 有多个 local minima，那么不能保证它会找到 global minimum

因此，为了克服这些困难，开发了一种 **stochastic version** 名为 **incremental gradient descent rule**。

gradient descent rule 在计算**所有** training examples 中累积的错误后更新权重；而 incremental gradient descent rule 通过在**每个** training example 后更新权重来使梯度下降的误差减少。

![W34 Incremental gradient descent](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W34 Incremental gradient descent.png)

## Incremental Gradient Descent Learning Algorithm
![W34 Incremental Gradient Descent Learning](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W34 Incremental Gradient Descent Learning.png)

![W34 Incremental Gradient Descent Learning 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W34 Incremental Gradient Descent Learning 2.png)

注：这里就没有对 $$\Delta w$$ 进行累积，因为 incremental gradient descent 只考虑单个点。

# Sigmoidal Perceptrons

![W34 Sigmoidal Perceptrons](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W34 Sigmoidal Perceptrons.png)

![W34 Sigmoidal Perceptrons 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W34 Sigmoidal Perceptrons 2.png)

![W34 Sigmoidal Perceptrons 3](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W34 Sigmoidal Perceptrons 3.png)

## Gradient Descent Learning Algorithm for Sigmoidal Perceptrons

![W34 Gradient Descent Learning Algorithm for Sigmoidal Perceptrons](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W34 Gradient Descent Learning Algorithm for Sigmoidal Perceptrons.png)

Example:

![W34 example 3](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W34 example 3.png)

![W34 example 4](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W34 example 4.png)

# Perceptron vs. Gradient Descent

Gradient descent finds the decision boundary which minimizes the **sum squared error** of the (target -net) value rather than the (target -output) value.

* Perceptron rule will find the decision boundary which minimizes the classification error – if the problem is linearly separable
* Gradient descent decision boundary may leave more instances misclassified as compared to the perceptron rule: could have a higher misclassification rate than with the perceptron rule

Perceptron rule (target - thresholded output) guaranteed to converge to a separating hyperplane if the problem is linearly separable.

# Batch vs incremental learning

![W34 Batch vs incremental learning](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W34 Batch vs incremental learning.png)

