# INT301 W9

## Radial-basis function networks

### Introduction

神经网络的可以看作是一个曲线拟合 (curve-fitting) 或函数近似 (function approximation) 问题。

对于 curve-fitting (approximation in high-dimensional spaces) 问题，可以等价于找到 interpolating surface (插值平面) in the space。这叫 **Curve-fitting** or **interpolation problem**。

注：实际问题中我们往往得不到一个函数关系具体的表达式，但往往可以通过观测等手段得到其在一些点处的函数值，这时，就需要用到插值的思想，通过对某些点已知的函数 (basis function) 值构造插值函数，用插值函数代替原函数来研究问题。

### Radial-Basis Functions

Radial-basis functions (径向基函数) 可以按照以下形式构建插值函数 F：
$$
F(\mathrm{x})=\sum_{i=1}^{N} w_{i} \phi\left(\left\|\mathrm{x}-\mathrm{x}_{i}\right\|\right)
$$
$\phi\left(\left\|\mathrm{x}-\mathrm{x}_{i}\right\|\right)$ 是一组非线性径向基函数 (nonlinear radial-basis functions)，$\mathrm{x}_{i}$ 是这些函数的中心，$\|\bullet\|$ 是 Euclidean norm。

![W9 RBF](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W9 RBF.png)

## Regularization Networks

Regularization network 将插值函数 F 建模为多元高斯函数的线性叠加 (linear superposition of multivariate Gaussian functions，其中 Gaussian functions 是常见的 basis function)，其中叠加的数量等于 input examples 的数量 N:
$$
F(x)=\sum_{i=1}^{N} w_{i} \exp \left(-\left\|x-x_{i}\right\|^{2} / 2 \sigma_{i}^{2}\right)
$$
这是一个**通用近似器 (universal approximator)**，给定足够多的 units，它可以任意地近似紧集 (compact set) 上的任何多元连续函数。

## RBF Network Structure

![W9 RBF network](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W9 RBF network.png)

An RBF network architecture consists of 3 layers:

* input layer: which passes the example vectors to the next layers
* hidden layer: applies a non-linear transformation functionto the inputs, and expands them in the usually very high-dimensional hidden space
* output layer: applies a linear transformation to the given vector

### RBF Network: Rationale

非线性函数可以将困难的非线性插值问题转换为涉及线性映射的更简单的问题。这就是为什么 RBF network 被设计出来。RBF 执行从输入空间到隐藏空间的非线性映射，然后执行从隐藏空间到输出空间的线性映射。

## Function Mapping by RBF Networks

RBF network 将插值函数 F 建模为有限的多元高斯函数的线性叠加：
$$
F(\mathrm{x})=\sum_{i=1}^{n} w_{i} \exp \left(-\frac{\left\|\mathrm{x}-\mathrm{t}_{i}\right\|^{2}}{2 \sigma_{i}^{2}}\right)
$$
其中：$w_{i}$ 是权重，basis function 的数量 n 少于 training set 中点的数量 (i.e. n< N)，basis function 的中心 $\mathrm{t}_{i}$ 是提前确定的。

## RBF Learning

![W9 RBF learning 1](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W9 RBF learning 1.png)

注：$x_{i}$ 相当于 training example，$d_{i}$ 相当于 label



![W9 RBF learning 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W9 RBF learning 2.png)

![W9 RBF learning 3](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W9 RBF learning 3.png)

![W9 RBF learning 4](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W9 RBF learning 4.png)

虽然这里让 W 和 $\Phi$ 以及 $d$ 挂钩了，但更新权重的时候只更新 W，和其他的没关系。

注：$\Phi^{+}$ 是 $\Phi$ 的 pseudo-inverse (伪逆)，即 $\Phi^{+} \times \Phi = 1$

![W9 RBF learning 5](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W9 RBF learning 5.png)

## RBF Training Algorithm

![W9 RBF Training Algorithm 1](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W9 RBF Training Algorithm 1.png)

![W9 RBF Training Algorithm 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W9 RBF Training Algorithm 2.png)

注：径向基函数的中心 $t_{i}$ 和方差 $\sigma_{i}^{2}$ 可以与输出权重一起训练，但这需要很长时间。因此，它们通常是事先确定的。

### Implementing RBF Networks

![W9 Implementing RBF Networks 1](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W9 Implementing RBF Networks 1.png)

![W9 Implementing RBF Networks 2](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W9 Implementing RBF Networks 2.png)

注：因为是二分类，因此选两个 centers。

![W9 Implementing RBF Networks 3](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W9 Implementing RBF Networks 3.png)

![W9 Implementing RBF Networks 4](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W9 Implementing RBF Networks 4.png)

## Finding RBF Centers

### Random Selection

最简单的方法是从给定的 training example 中随机选择中心。

### K-means Clustering

使用 K 均值聚类算法选择中心时，假设相似的输入会产生相似的输出，我们将这些相似的输入对捆绑在一起形成聚类。K 均值聚类仅将 RBF 中心放置在输入空间中存在大量训练示例的区域中。

#### Cluster

Cluster：数据的集合，同一集群中元素彼此相似，不同集群中的元素不同。

Cluster analysis：无监督学习，根据数据中发现的特征查找数据之间的相似性，并将相似的数据对象分组到聚类中。

A good clustering method will produce high quality clusters with：

* high intra-class similarity
  similar to one another within the same cluster
* low inter-class similarity
  dissimilar to the objects in other clusters

聚类方法的质量也是通过其发现部分或全部的 hidden patterns 的能力来衡量的。

#### Similarity and Dissimilarity

Distances are normally used to measure the similarity or dissimilarity between two data objects。

##### Minkowski distance

$$
d(i, j)=\sqrt[q]{\left(\left|x_{i 1}-x_{j 1}\right|^{q}+\left|x_{i 2}-x_{j 2}\right|^{q}+\ldots+\left|x_{i p}-x_{j p}\right|^{q}\right)}
$$

where $i= (x_{i1}, x_{i2}, …, x_{ip})$ and $j= (x_{j1}, x_{j2}, …, x_{jp})$ are two p-dimensional data objects, q is a positive integer。

If $q=1, d$ is Manhattan distance 
$$
d(i, j)=\left|x_{i 1}-x_{j 1}\right|+\left|x_{i 2}-x_{j 2}\right|+\ldots+\left|x_{i p}-x_{j p}\right|
$$
If $q=2, d$ is Euclidean distance: 
$$
d(i, j)=\sqrt{\left(\left|x_{i 1}-x_{j 1}\right|^{2}+\left|x_{i 2}-x_{j 2}\right|^{2}+\ldots+\left|x_{i p}-x_{j p}\right|^{2}\right)}
$$

#### K-means clustering algorithm

该算法将数据点划分为 K 个不相交的子集。

聚类分类标准：

* 聚类中心设置在数据的高密度区域
* 数据点被分配到与中心具有最小距离的聚类

$$
J=\sum_{j=1}^{K} \sum_{n \in S_{j}}\left\|\mathbf{x}_{n}-\mathbf{c}_{j}\right\|^{2}
$$
where
$S_{j}:$ the $j$-th cluster containing $N_{j}$ data points
$\mathbf{c}_{j}=\frac{1}{N_{j}} \sum_{n \in S_{j}} \mathbf{x}_{n}:$ the mean of the data points in cluster $S_{j}$

注：$x_{n}$ 是 $j$-th cluster 中的点，$c_{j}$ 是 $j$-th cluster 的中心，$\sum_{n \in S_{j}}\left\|\mathbf{x}_{n}-\mathbf{c}_{j}\right\|^{2}$ 计算 $j$-th cluster 中所有点到中心的距离，因此 $J$ 就是所有 cluster 中点到到中心点距离的总和，因此我们的目的是最小化 $J$。

#### Algorithm process：

1，随机选择 K 个中心点。计算 training examples 中的每个点到这 K 个中心点的距离，把每个点点分到离它最近的聚类中；

2，根据同一个聚类中的所有点的坐标，计算出该类的 mean，作为新的中心点；

3，再计算 training examples 中的每个点到这 K 个新中心点的距离，把每个点点分到离它最近的聚类中；

4，如果第 3 步中没有出现分类的变化，则停止算法；否则，回到第 2 步。

![W9 K-means](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W9 K-means.png)



#### Issues

此方法不适用于具有非凸形状的聚类，并且对噪声和异常值元素敏感。

![W9 Kmeans issues](D:\Files\Learning Materials\Y4\INT301-BioComputation\Images for md\W9 Kmeans issues.png)

注：上图左边是非凸形状，右边是噪声和异常值；其中每个图的左边是 kmeans 算法，右边是其他算法。

#### Improve k-means
Repeated k-means：

* Try several random initializations and take the best.

Better initialization：

* Use some better heuristic to allocate the initial distribution of code vectors.
  * Designing good initialization is not any easier than designing good clustering algorithm at the first place.

## RBF Networks vs. MLP

### Similarities

* The RBF Networks and the MLP are **layered feedforward networks** that produce **nonlinear function mappings**

* They are both proven to be **universal approximators**

### Differences

* RBF 网络只有一个隐藏层，而 MLP 网络有一个或多个隐藏层，具体取决于应用任务
* MLP 的隐藏层和输出层中的节点使用相同的激活函数，而 RBF 在每个节点上使用不同的激活函数（使用不同的中心和方差）
* MLP 的隐藏层和输出层都是非线性的，而 RBF 只有隐藏层是非线性的（输出层是线性的）
* RBF 节点中的激活函数计算输入示例和中心之间的欧氏距离，而MLP 的激活函数计算输入示例和传入权重的内积
* MLP constructs **global approximations** while RBF construct **local approximations**

